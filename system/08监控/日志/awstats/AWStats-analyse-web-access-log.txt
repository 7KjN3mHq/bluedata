用 AWStats 分析网站访问日志


日志文件的处理：
对于 Apache:
CentOS:
yum -y install cronolog

Debian:
yum -y install cronolog

vi /etc/httpd/conf/httpd.conf
CustomLog "|/usr/sbin/cronolog /var/log/httpd/access_log.%Y%m%d" combined

清除历史日志：
vi /etc/crontab
0 0 * * * root find /var/log/httpd -type f -mtime +1 -exec rm {} \;

对于 Nginx:
vi /usr/local/nginx/conf/nginx.conf
access_log  /var/log/nginx/access_log  combined;

按天截断日志：
vi /etc/crontab
59 23 * * * root cd /var/log/nginx ; mv -f access_log access_log.1 ; kill -USR1 `cat /var/run/nginx.pid`

多台服务器日志合并问题：
把多个日志中的记录按时间排序后合并成一个文件：
sort -m -t " " -k 4 -o access_log.`date +%Y%m%d --date='1 days ago'` access_log.web1 access_log.web2 access_log.web3
注释： -m 即使用 merge 优化算法。


安装 AWStats:
http://awstats.sourceforge.net/
wget http://prdownloads.sourceforge.net/awstats/awstats-6.9.tar.gz
tar zxvf awstats-6.9.tar.gz
chown -R root:root awstats-6.9
mv awstats-6.9 /usr/local/awstats

搜索引擎的补充：
wget http://www.chedong.com/tech/lib.tgz
tar zxvf lib.tgz
mv lib/*.pm /usr/local/awstats/wwwroot/cgi-bin/lib/
注：蜘蛛定义部分增加了区分 Yahoo! 中国， Soso ，豆瓣，鲜果等，其他的是几个国外的 RSS 阅读器；搜索引擎部分区分了百度图片，有道搜索，Soso 。

增加百度和谷歌的查询链接：
vi /usr/local/awstats/wwwroot/cgi-bin/awstats.pl
在 15367 15417 18757 18829 行后增加：
. " <a target=\"_blank\" href=\"http://www.baidu.com/s?wd=site:" .$SiteDomain. " ". $mot ."\">[Baidu]</a>"
. " <a target=\"_blank\" href=\"http://www.google.cn/search?ie=gbk&q=site:" .$SiteDomain. " " . $mot ."\">[Google]</a>"

如果 URL 数量非常大，而内存又足够的话，可以增大 $LIMITFLUSH ( 默认为 5000) 提升性能：
vi /usr/local/awstats/wwwroot/cgi-bin/awstats.pl
$LIMITFLUSH=50000;


基本配置：
cd /usr/local/awstats/wwwroot/cgi-bin
mv awstats.model.conf common.conf
chmod 644 common.conf

解决中文搜索引擎统计问题：
vi common.conf
LoadPlugin="decodeutfkeys"

需要安装 Perl URI 模块：
CentOS:
yum -y install perl-URI

Debian:
apt-get -y install liburi-perl


创建 data 目录用于统计数据输出：
mkdir -p /home/awstats/data


安装插件：
GeoIP
CentOS:
yum -y install GeoIP perl-Geo-IP

Debian:
apt-get -y install libgeo-ip-perl

只使用 Perl 则只需安装 perl-Geo-IPfree ，但不能同时使用。

下载 GeoIP 数据库：
wget http://geolite.maxmind.com/download/geoip/database/GeoLiteCountry/GeoIP.dat.gz
gzip -d GeoIP.dat.gz
cp GeoIP.dat /usr/local/awstats/wwwroot/cgi-bin/plugins/

在配置文件中启用插件：
vi common.conf
LoadPlugin="geoip GEOIP_STANDARD /usr/local/awstats/wwwroot/cgi-bin/plugins/GeoIP.dat"

qqhostinfo
GeoLiteCity 的城市信息不准确，使用 QQ 纯真版 IP 数据库 分析 IP 地址。
下载 纯真版 IP 数据库：
http://www.cz88.net/fox/
解压后得到 QQWry.Dat 。

下载 qqhostinfo.pm 和 qqwry.pl 用于分析 QQWry.dat:
wget http://www.ieasy.org/download/qqhostinfo.pm
wget http://www.ieasy.org/download/qqwry.pl
vi qqwry.pl
把 ./QQWry.Dat 改为：
${DIR}/plugins/QQWry.Dat

cp QQWry.Dat qqhostinfo.pm qqwry.pl /usr/local/awstats/wwwroot/cgi-bin/plugins/

使用 qqhostinfo 插件需要安装 Perl Net::XWhois 模块：
CentOS:
yum -y install perl-Net-XWhois

Debian:
apt-get -y install libnet-xwhois-perl

在配置文件中启用插件：
vi common.conf
LoadPlugin="qqhostinfo"

增加对 BaiDuSpider/Googlebot/Yahoo!Slurp/MSNBot/SogouSpider 的详细统计：
vi common.conf
ExtraSectionName1="Baidu crawls - Top 50"
ExtraSectionCodeFilter1="200 304"
ExtraSectionCondition1="UA,(.*Baiduspider.*)"
ExtraSectionFirstColumnValues1="URL,(.*)"
ExtraSectionFirstColumnFormat1="%s"
ExtraSectionStatTypes1=PHBL
ExtraSectionAddAverageRow1=0
ExtraSectionAddSumRow1=1
MaxNbOfExtra1=50
MinHitExtra1=1

ExtraSectionName2="Google crawls - Top 50"
ExtraSectionCodeFilter2="200 304"
ExtraSectionCondition2="UA,(.*Googlebot.*)"
ExtraSectionFirstColumnValues2="URL,(.*)"
ExtraSectionFirstColumnFormat2="%s"
ExtraSectionStatTypes2=PHBL
ExtraSectionAddAverageRow2=0
ExtraSectionAddSumRow2=2
MaxNbOfExtra2=50
MinHitExtra2=1

ExtraSectionName3="Yahoo Slurp crawls - Top 50"
ExtraSectionCodeFilter3="200 304"
ExtraSectionCondition3="UA,(.*Slurp.*)"
ExtraSectionFirstColumnValues3="URL,(.*)"
ExtraSectionFirstColumnFormat3="%s"
ExtraSectionStatTypes3=PHBL
ExtraSectionAddAverageRow3=0
ExtraSectionAddSumRow3=3
MaxNbOfExtra3=50
MinHitExtra3=1

ExtraSectionName4="MSN crawls - Top 50"
ExtraSectionCodeFilter4="200 304"
ExtraSectionCondition4="UA,(.*msnbot.*)"
ExtraSectionFirstColumnValues4="URL,(.*)"
ExtraSectionFirstColumnFormat4="%s"
ExtraSectionStatTypes4=PHBL
ExtraSectionAddAverageRow4=0
ExtraSectionAddSumRow4=4
MaxNbOfExtra4=50
MinHitExtra4=1

ExtraSectionName5="Sohu crawls - Top 50"
ExtraSectionCodeFilter5="200 304"
ExtraSectionCondition5="UA,(.*sogou.*)"
ExtraSectionFirstColumnValues5="URL,(.*)"
ExtraSectionFirstColumnFormat5="%s"
ExtraSectionStatTypes5=PHBL
ExtraSectionAddAverageRow5=0
ExtraSectionAddSumRow5=5
MaxNbOfExtra5=50
MinHitExtra5=1

如果扩展配置统计的条目较多（比如：蜘蛛抓取的不同 URL 的个数很多），就需要在全局扩大一下 ExtraTrackedRowsLimit 这个参数：
ExtraTrackedRowsLimit=50000


创建站点配置文件：
vi awstats.domain.com.conf
Include "common.conf"
LogFile="/var/log/httpd/access_log.%YYYY-24%MM-24%DD-24"
SiteDomain="www.domain.com"
HostAliases="domain.com"
DirData="/home/awstats/data/domain.com/"

# Nginx 日志
LogFile="/var/log/nginx/access_domain.com.log.1"

# 统计多个日志文件
LogFile="/usr/local/awstats/tools/logresolvemerge.pl /var/log/httpd/access_log* |"


分析日志：
/usr/local/awstats/wwwroot/cgi-bin/awstats.pl -update -config=domain.com
默认是按月生成归档数据，如果日志文件过大会导致统计过慢或失败，
可以加上 -databasebreak=day 参数按天进行统计，报表输出时还需增加 &databasebreak=day&day=DD 。

按天统计出的归档日志还是过大的话，可生成静态文件：
/usr/bin/perl /usr/local/awstats/tools/awstats_buildstaticpages.pl -update -config=domain.com -lang=cn -month=`date +%m --date='1 days ago'` -dir=/var/www/awstats/html/ -awstatsprog=/usr/local/awstats/wwwroot/cgi-bin/awstats.pl
不能使用 -databasebreak=day 参数。
-month 指定按月统计，否则不能生成每月最后一天的静态文件。

如果日志文件实在太大，可考虑不记录图片 css js 的访问日志：
location ~* ^.+.(jpg|jpeg|gif|png|bmp|css|js)$ {
access_log  off;
}

批量分析脚本：
vi /usr/local/bin/awstats.sh
#!/bin/sh
for sitedomain in `ls /var/log/nginx/ | grep log$ | grep ^access | cut -d "_" -f 2 | sed 's/\.log//'`; do
  /usr/local/awstats/wwwroot/cgi-bin/awstats.pl -update -config=$sitedomain -LogFile=access_$sitedomain.log
done

chmod +x /usr/local/bin/awstats.sh

使用 logrotate 回滚日志：
vi /usr/local/nginx/conf/logrotate.conf
/var/log/nginx/*.log {
    daily
    missingok
    rotate 7
    compress
    delaycompress
    notifempty
    create 640 www-data adm
    sharedscripts
    postrotate
        if [ -f /var/run/nginx.pid ]; then
          kill -USR1 `cat /var/run/nginx.pid`
        fi
        for sitedomain in `ls /var/log/nginx/ | grep log$ | grep ^access | cut -d "_" -f 2 | sed 's/\.log//'`; do
          /usr/local/awstats/wwwroot/cgi-bin/awstats.pl -update -config=$sitedomain > /dev/null 2>&1
        done
    endscript
}

调试：
logrotate -d /usr/local/nginx/conf/logrotate.conf

执行：
logrotate -f /usr/local/nginx/conf/logrotate.conf

定期执行：
vi /etc/crontab
0 0 * * * root logrotate -f /usr/local/nginx/conf/logrotate.conf

Bug:
加上 -LogFile=access_domain.com.log.1 参数在 crontab 中不能执行。


查看报表：
Apache 下创建 AWStats 的虚拟主机：
vi /etc/httpd/conf.d/vhosts_awstats.conf
<VirtualHost *:80>
DocumentRoot /usr/local/awstats/wwwroot
ServerName awstats.domain.com
<Directory "/usr/local/awstats/wwwroot">
    Options None
    AllowOverride None
    Order allow,deny
    Allow from all
    AuthName "AWStats Access"
    AuthType Basic
    AuthUserFile /usr/local/awstats/htpasswd.users
    Require valid-user
</Directory>
</VirtualHost>

Nginx 下创建 AWStats 的虚拟主机：
server {
    include  port.conf;
    server_name  awstats.domain.com;
    root  /usr/local/awstats/wwwroot;
    index  index.php;
    location ~ .*\.php?$ {
        fastcgi_pass  127.0.0.1:9000;
        fastcgi_param  SCRIPT_FILENAME  $document_root$fastcgi_script_name;
        include  fastcgi_params;
        auth_basic  "AWStats Access";
        auth_basic_user_file  /usr/local/awstats/htpasswd.users;
    }
    location /cgi-bin/awstats.pl {
        fastcgi_pass  127.0.0.1:8999;
        fastcgi_param  SCRIPT_FILENAME  $document_root$fastcgi_script_name;
        include  fastcgi_params;
        auth_basic  "AWStats Access";
        auth_basic_user_file  /usr/local/awstats/htpasswd.users;
    }
}


htpasswd -c /usr/local/awstats/htpasswd.users awstats

查看：
http://awstats.domain.com/cgi-bin/awstats.pl?framename=mainright&config=domain.com&year=2009&month=08


用 PHP Calendar 做日历浏览界面：
http://www.cascade.org.uk/software/php/calendar/
wget http://www.cascade.org.uk/software/php/calendar/Calendar.txt
mv Calendar.txt calendar.php
然后创建 index.php 程序文件并放到 wwwroot 目录下：
cp calendar.php index.php /usr/local/awstats/wwwroot/