yum -y install kmod-drbd82-smp drbd82

/usr/share/doc/drbd82-8.2.6/drbd.conf

vi /etc/drbd.conf
resource data {
    protocol C;
    net {
        after-sb-0pri discard-older-primary;
        after-sb-1pri call-pri-lost-after-sb;
        after-sb-2pri call-pri-lost-after-sb;
    }
    on centos4-1 {
        device    /dev/drbd0;
        disk      /dev/hda7;
        address   192.168.0.171:8000;
        meta-disk internal;
    }
    on centos4-2 {
        device    /dev/drbd0;
        disk      /dev/hda7;
        address   192.168.0.172:8000;
        meta-disk internal;
    }
}
resource home {
    protocol C;
    net {
        after-sb-0pri discard-older-primary;
        after-sb-1pri call-pri-lost-after-sb;
        after-sb-2pri call-pri-lost-after-sb;
    }
    on centos4-1 {
        device    /dev/drbd1;
        disk      /dev/hda8;
        address   192.168.0.171:8001;
        meta-disk internal;
   }
   on centos4-2 {
        device    /dev/drbd1;
        disk      /dev/hda8;
        address   192.168.0.172:8001;
        meta-disk internal;
   }
}

两台机器都执行：
drbdadm create-md data
drbdadm create-md home

可能会出现这样的错误：
v08 Magic number not found
md_offset 3010412544
al_offset 3010379776
bm_offset 3010285568

Found ext3 filesystem which uses 2939860 kB
current configuration leaves usable 2939732 kB

Device size would be truncated, which
would corrupt data and result in
'access beyond end of device' errors.
You need to either
   * use external meta data (recommended)
   * shrink that filesystem first
   * zero out the device (destroy the filesystem)
Operation refused.

Command 'drbdmeta /dev/drbd0 v08 /dev/hda7 internal create-md' terminated with exit code 40
drbdadm aborting

解决办法：
dd if=/dev/zero bs=1M count=1 of=/dev/hda7; sync
drbdadm create-md data

在其中一个节点上：
drbdadm adjust data
drbdsetup /dev/drbd0 primary -o

然后都开启服务：
/etc/init.d/drbd start

创建文件系统后就可以挂载使用了
mkfs -t ext3 /dev/drbd0


LVM
vi /etc/drbd.conf
resource r0 {
    protocol C;
    net {
        after-sb-0pri discard-older-primary;
        after-sb-1pri call-pri-lost-after-sb;
        after-sb-2pri call-pri-lost-after-sb;
    }
    on app1 {
        device    /dev/drbd0;
        disk      /dev/cciss/c0d0p7;
        address   192.168.0.201:7789;
        meta-disk internal;
    }
    on app2 {
        device    /dev/drbd0;
        disk      /dev/cciss/c0d0p7;
        address   192.168.0.202:7789;
        meta-disk internal;
    }
}

在两个节点上创建资源并启动 drbd
dd if=/dev/zero bs=1M count=1 of=/dev/cciss/c0d0p7 ; sync
drbdadm create-md r0
/etc/init.d/drbd start

配置 LVM
vi /etc/lvm/lvm.conf
# filter = [ "a/.*/" ]
filter = [ "r|/dev/cdrom|", "r|/dev/cciss/c0d0p7|", "a|/dev/drbd0|" ]

把一个节点设置为主节点
drbdsetup /dev/drbd0 primary -o

然后创建 pv/vg/lv
pvcreate /dev/drbd0
vgcreate vg /dev/drbd0
lvcreate -L 36G -n data vg
lvcreate -L 72G -n home vg

mkfs -t ext3 /dev/vg/data
mkfs -t ext3 /dev/vg/home

mount -t ext3 /dev/vg/data /data
mount -t ext3 /dev/vg/home /home

cd /data
echo "test" > test

umount /data
umount /home
vgchange -a n vg
drbdadm secondary r0


drbdadm primary r0
vgscan    # 第一次需要，以后就不用再 vgscan 了。
vgchange -a y vg
mount -t ext3 /dev/vg/data /data
mount -t ext3 /dev/vg/home /home

cat /data/test
test

cd /home
echo "test" > test

交换上面的步骤，在 node1 的 /home 目录下应该也能看到 test 文件。

调整容量：
umount /data
lvextend -L +18G /dev/vg/data
e2fsck -f /dev/vg/data
resize2fs /dev/vg/data

mount -t ext3 /dev/vg/data /data
/data 下的 test 文件还在。

进行主从交换。在 node2 上可以看到 /dev/vg/data 的容量已经扩大了。

heartbeat 的配置：
vi /etc/ha.d/haresources
app1 \
IPaddr::192.168.0.250/24/eth1 \
drbddisk::r0 \
LVM::vg \
Filesystem::/dev/vg/data::/data::ext3 \
Filesystem::/dev/vg/home::/home::ext3


       wfc-timeout time
              Wait for connection timeout.  The init script  drbd(8)
              blocks  the  boot process until the DRBD resources are
              connected.  This is so when the cluster manager starts
              later, it does not see a resource with internal split-
              brain.  In case you want to limit the wait time, do it
              here.   Default  is  0, which means unlimited. Unit is
              seconds.

       degr-wfc-timeout time
              Wait for  connection  timeout,  if  this  node  was  a
              degraded cluster.  In case a degraded cluster (= clus-
              ter with only one node left) is rebooted, this timeout
              value is used instead of wfc-timeout, because the peer
              is less likely to show up in time, if it had been dead
              before.  Default is 60, unit is seconds. Value 0 means
              unlimited.


/home 192.168.0.0/24(rw,no_root_squash,async)
/usr/local/resin-2.1.17 192.168.0.0/24(rw,no_root_squash,async)